{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cpu.html\n!pip install git+https://github.com/nishant42491/PyGCL.git\n!pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html\n","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:31:23.950712Z","iopub.execute_input":"2022-11-20T13:31:23.951140Z","iopub.status.idle":"2022-11-20T13:32:30.809438Z","shell.execute_reply.started":"2022-11-20T13:31:23.951058Z","shell.execute_reply":"2022-11-20T13:32:30.807849Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-1.11.0+cpu.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcpu/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (292 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.0/292.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcpu/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (658 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.2/658.2 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcpu/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (317 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.7/317.7 kB\u001b[0m \u001b[31m459.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch-spline-conv\n  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcpu/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.8/140.8 kB\u001b[0m \u001b[31m283.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch-geometric\n  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.0/467.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse) (1.7.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.21.6)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.28.1)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.26.12)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (3.1.0)\nBuilding wheels for collected packages: torch-geometric\n  Building wheel for torch-geometric (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=d751c9b6f501e9f8a0fcf132c01f2e91b15e4eff4572554d873af25b2782c7c3\n  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\nSuccessfully built torch-geometric\nInstalling collected packages: torch-spline-conv, torch-scatter, torch-cluster, torch-sparse, torch-geometric\nSuccessfully installed torch-cluster-1.6.0 torch-geometric-2.1.0.post1 torch-scatter-2.0.9 torch-sparse-0.6.15 torch-spline-conv-1.2.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/nishant42491/PyGCL.git\n  Cloning https://github.com/nishant42491/PyGCL.git to /tmp/pip-req-build-5zczlies\n  Running command git clone --filter=blob:none --quiet https://github.com/nishant42491/PyGCL.git /tmp/pip-req-build-5zczlies\n  Resolved https://github.com/nishant42491/PyGCL.git to commit 857ad38586125e425a3a9a6c87227d9beb6cfe1a\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (1.11.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (1.21.6)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (2.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (1.7.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (4.64.0)\nRequirement already satisfied: torch-geometric>=1.7 in /opt/conda/lib/python3.7/site-packages (from PyGCL==0.1.2) (2.1.0.post1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.9->PyGCL==0.1.2) (4.4.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=1.7->PyGCL==0.1.2) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=1.7->PyGCL==0.1.2) (2.28.1)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from torch-geometric>=1.7->PyGCL==0.1.2) (3.0.9)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->PyGCL==0.1.2) (5.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->PyGCL==0.1.2) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->PyGCL==0.1.2) (1.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->torch-geometric>=1.7->PyGCL==0.1.2) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=1.7->PyGCL==0.1.2) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=1.7->PyGCL==0.1.2) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=1.7->PyGCL==0.1.2) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric>=1.7->PyGCL==0.1.2) (2.1.0)\nBuilding wheels for collected packages: PyGCL\n  Building wheel for PyGCL (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyGCL: filename=PyGCL-0.1.2-py3-none-any.whl size=32827 sha256=f70ab2b960feecd724338a764193ae233c4e0a2a75499b4eea06354b47ce9a95\n  Stored in directory: /tmp/pip-ephem-wheel-cache-x8p0ys7s/wheels/cd/0a/ec/5855a3bdf24ece7d4630a3bd404fac6f14c177e2a77bb93b83\nSuccessfully built PyGCL\nInstalling collected packages: PyGCL\nSuccessfully installed PyGCL-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in links: https://data.dgl.ai/wheels/repo.html\nCollecting dgl\n  Downloading https://data.dgl.ai/wheels/dgl-0.9.1.post1-cp37-cp37m-manylinux1_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting dglgo\n  Downloading dglgo-0.0.2-py3-none-any.whl (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 kB\u001b[0m \u001b[31m439.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from dgl) (4.64.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from dgl) (1.7.3)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from dgl) (1.21.6)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from dgl) (2.28.1)\nRequirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.7/site-packages (from dgl) (2.5)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.7/site-packages (from dgl) (5.9.1)\nCollecting ogb>=1.3.3\n  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from dglgo) (1.0.2)\nRequirement already satisfied: typer>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from dglgo) (0.4.2)\nCollecting rdkit-pypi\n  Downloading rdkit_pypi-2022.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pydantic>=1.9.0\n  Downloading pydantic-1.10.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: autopep8>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from dglgo) (1.6.0)\nCollecting ruamel.yaml>=0.17.20\n  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.7/site-packages (from dglgo) (6.0)\nRequirement already satisfied: isort>=5.10.1 in /opt/conda/lib/python3.7/site-packages (from dglgo) (5.10.1)\nCollecting numpydoc>=1.1.0\n  Downloading numpydoc-1.5.0-py3-none-any.whl (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pycodestyle>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from autopep8>=1.6.0->dglgo) (2.8.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from autopep8>=1.6.0->dglgo) (0.10.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.1->dgl) (5.1.1)\nCollecting sphinx>=4.2\n  Downloading sphinx-5.3.0-py3-none-any.whl (3.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: Jinja2>=2.10 in /opt/conda/lib/python3.7/site-packages (from numpydoc>=1.1.0->dglgo) (3.1.2)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb>=1.3.3->dglgo) (1.3.5)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb>=1.3.3->dglgo) (1.26.12)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb>=1.3.3->dglgo) (1.15.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from ogb>=1.3.3->dglgo) (1.11.0+cpu)\nCollecting outdated>=0.2.0\n  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.7/site-packages (from pydantic>=1.9.0->dglgo) (4.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->dgl) (2022.9.24)\nCollecting ruamel.yaml.clib>=0.2.6\n  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->dglgo) (1.0.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer>=0.4.0->dglgo) (8.0.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit-pypi->dglgo) (9.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer>=0.4.0->dglgo) (4.13.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.1.1)\nCollecting littleutils\n  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.7/site-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (59.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.1)\nCollecting sphinxcontrib-htmlhelp>=2.0.0\n  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.5/100.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sphinxcontrib-qthelp\n  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.7/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (21.3)\nCollecting sphinxcontrib-applehelp\n  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: docutils<0.20,>=0.14 in /opt/conda/lib/python3.7/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (0.19)\nCollecting imagesize>=1.3\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nCollecting sphinxcontrib-jsmath\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nCollecting sphinxcontrib-devhelp\n  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: Pygments>=2.12 in /opt/conda/lib/python3.7/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.12.0)\nCollecting alabaster<0.8,>=0.7\n  Downloading alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.5\n  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.7/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.7/site-packages (from sphinx>=4.2->numpydoc>=1.1.0->dglgo) (2.10.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9.0.0,>=7.1.1->typer>=0.4.0->dglgo) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=21.0->sphinx>=4.2->numpydoc>=1.1.0->dglgo) (3.0.9)\nBuilding wheels for collected packages: littleutils\n  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=b02302c4e2b5606d5fd175381e57f0c4183a5c69bcd92ad79b3e505f8086ef50\n  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\nSuccessfully built littleutils\nInstalling collected packages: littleutils, alabaster, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, ruamel.yaml.clib, rdkit-pypi, pydantic, imagesize, sphinx, ruamel.yaml, outdated, dgl, ogb, numpydoc, dglgo\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.8.2\n    Uninstalling pydantic-1.8.2:\n      Successfully uninstalled pydantic-1.8.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.0.17 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 1.10.2 which is incompatible.\nthinc 8.0.17 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\nspacy 3.3.1 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 1.10.2 which is incompatible.\nspacy 3.3.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nconfection 0.0.3 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed alabaster-0.7.12 dgl-0.9.1.post1 dglgo-0.0.2 imagesize-1.4.1 littleutils-0.2.2 numpydoc-1.5.0 ogb-1.3.5 outdated-0.2.2 pydantic-1.10.2 rdkit-pypi-2022.9.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 sphinx-5.3.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchmetrics","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:30.812884Z","iopub.execute_input":"2022-11-20T13:32:30.813264Z","iopub.status.idle":"2022-11-20T13:32:39.718758Z","shell.execute_reply.started":"2022-11-20T13:32:30.813232Z","shell.execute_reply":"2022-11-20T13:32:39.717974Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.10.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.4.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.3.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.11.0+cpu)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"'''import random\nrandom.seed(10)\nimport numpy as np\nnp.random.seed(10)\nimport torch\ntorch.manual_seed(10)'''","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:39.719728Z","iopub.execute_input":"2022-11-20T13:32:39.720025Z","iopub.status.idle":"2022-11-20T13:32:39.730291Z","shell.execute_reply.started":"2022-11-20T13:32:39.719980Z","shell.execute_reply":"2022-11-20T13:32:39.729313Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'import random\\nrandom.seed(10)\\nimport numpy as np\\nnp.random.seed(10)\\nimport torch\\ntorch.manual_seed(10)'"},"metadata":{}}]},{"cell_type":"code","source":"test_acc = []","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:39.732889Z","iopub.execute_input":"2022-11-20T13:32:39.733299Z","iopub.status.idle":"2022-11-20T13:32:39.825389Z","shell.execute_reply.started":"2022-11-20T13:32:39.733260Z","shell.execute_reply":"2022-11-20T13:32:39.824532Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test_acc","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:39.827381Z","iopub.execute_input":"2022-11-20T13:32:39.827740Z","iopub.status.idle":"2022-11-20T13:32:39.840050Z","shell.execute_reply.started":"2022-11-20T13:32:39.827704Z","shell.execute_reply":"2022-11-20T13:32:39.838711Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"a = 0\n\nif len(test_acc) > 1:\n    for i in test_acc:\n        a+=i\n\n    print(a/len(test_acc))\n","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:39.841482Z","iopub.execute_input":"2022-11-20T13:32:39.841890Z","iopub.status.idle":"2022-11-20T13:32:39.851780Z","shell.execute_reply.started":"2022-11-20T13:32:39.841859Z","shell.execute_reply":"2022-11-20T13:32:39.850641Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nimport torch\nimport torch_geometric.nn as nn\nimport numpy as np\nimport os\nfrom torch_geometric.utils import dense_to_sparse\nfrom torch_geometric.data import Data\nimport torch\nimport os.path as osp\nimport GCL.losses as L\nimport GCL.augmentors as A\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchmetrics import Accuracy,AUROC,F1Score\n\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom GCL.eval import get_split, LREvaluator\nfrom GCL.models import DualBranchContrast\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.datasets import Planetoid\nimport pytorch_lightning as pl\nimport wandb\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:39.853027Z","iopub.execute_input":"2022-11-20T13:32:39.853342Z","iopub.status.idle":"2022-11-20T13:32:46.677800Z","shell.execute_reply.started":"2022-11-20T13:32:39.853313Z","shell.execute_reply":"2022-11-20T13:32:46.676831Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"DGL backend not selected or invalid.  Assuming PyTorch for now.\n","output_type":"stream"},{"name":"stdout","text":"Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:46.679253Z","iopub.execute_input":"2022-11-20T13:32:46.680854Z","iopub.status.idle":"2022-11-20T13:32:48.618785Z","shell.execute_reply.started":"2022-11-20T13:32:46.680817Z","shell.execute_reply":"2022-11-20T13:32:48.617415Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb_logger = WandbLogger(project=\"Graph_Contrastive_Learning\", entity=\"nishant_\")","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:48.620580Z","iopub.execute_input":"2022-11-20T13:32:48.621067Z","iopub.status.idle":"2022-11-20T13:32:55.730086Z","shell.execute_reply.started":"2022-11-20T13:32:48.621003Z","shell.execute_reply":"2022-11-20T13:32:55.729262Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnishant_\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20221120_133248-14ken9ks</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/nishant_/Graph_Contrastive_Learning/runs/14ken9ks\" target=\"_blank\">celestial-frost-143</a></strong> to <a href=\"https://wandb.ai/nishant_/Graph_Contrastive_Learning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}}]},{"cell_type":"code","source":"cuda = True if torch.cuda.is_available() else False","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:55.735849Z","iopub.execute_input":"2022-11-20T13:32:55.736385Z","iopub.status.idle":"2022-11-20T13:32:55.743512Z","shell.execute_reply.started":"2022-11-20T13:32:55.736337Z","shell.execute_reply":"2022-11-20T13:32:55.742842Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def prepare_trte_data(data_folder, view_list):\n    num_view = len(view_list)\n    labels_tr = np.loadtxt(os.path.join(data_folder, \"labels_tr.csv\"), delimiter=',')\n    labels_te = np.loadtxt(os.path.join(data_folder, \"labels_te.csv\"), delimiter=',')\n    labels_tr = labels_tr.astype(int)\n    labels_te = labels_te.astype(int)\n    data_tr_list = []\n    data_te_list = []\n    for i in view_list:\n        data_tr_list.append(np.loadtxt(os.path.join(data_folder, str(i)+\"_tr.csv\"), delimiter=','))\n        data_te_list.append(np.loadtxt(os.path.join(data_folder, str(i)+\"_te.csv\"), delimiter=','))\n    num_tr = data_tr_list[0].shape[0]\n    num_te = data_te_list[0].shape[0]\n    data_mat_list = []\n    for i in range(num_view):\n        data_mat_list.append(np.concatenate((data_tr_list[i], data_te_list[i]), axis=0))\n    data_tensor_list = []\n    for i in range(len(data_mat_list)):\n        data_tensor_list.append(torch.FloatTensor(data_mat_list[i]))\n        if cuda:\n            data_tensor_list[i] = data_tensor_list[i].cuda()\n    idx_dict = {}\n    idx_dict[\"tr\"] = list(range(num_tr))\n    idx_dict[\"te\"] = list(range(num_tr, (num_tr+num_te)))\n    data_train_list = []\n    data_all_list = []\n    for i in range(len(data_tensor_list)):\n        data_train_list.append(data_tensor_list[i][idx_dict[\"tr\"]].clone())\n        data_all_list.append(torch.cat((data_tensor_list[i][idx_dict[\"tr\"]].clone(),\n                                       data_tensor_list[i][idx_dict[\"te\"]].clone()),0))\n    labels = np.concatenate((labels_tr, labels_te))\n    \n    return data_train_list, data_all_list, idx_dict, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:55.746374Z","iopub.execute_input":"2022-11-20T13:32:55.748087Z","iopub.status.idle":"2022-11-20T13:32:55.767613Z","shell.execute_reply.started":"2022-11-20T13:32:55.748057Z","shell.execute_reply":"2022-11-20T13:32:55.766073Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_folder = r'../input/graph-contrastive-learning-1/BRCA/BRCA'\nview_list = [1,2,3]","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:55.769392Z","iopub.execute_input":"2022-11-20T13:32:55.769839Z","iopub.status.idle":"2022-11-20T13:32:55.782101Z","shell.execute_reply.started":"2022-11-20T13:32:55.769805Z","shell.execute_reply":"2022-11-20T13:32:55.781184Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder, view_list)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:38:22.502497Z","iopub.execute_input":"2022-11-20T13:38:22.502918Z","iopub.status.idle":"2022-11-20T13:38:24.413515Z","shell.execute_reply.started":"2022-11-20T13:38:22.502883Z","shell.execute_reply":"2022-11-20T13:38:24.412203Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"cuda = True if torch.cuda.is_available() else False\n\ndef cal_sample_weight(labels, num_class, use_sample_weight=True):\n    if not use_sample_weight:\n        return np.ones(len(labels)) / len(labels)\n    count = np.zeros(num_class)\n    for i in range(num_class):\n        count[i] = np.sum(labels==i)\n    sample_weight = np.zeros(labels.shape)\n    for i in range(num_class):\n        sample_weight[np.where(labels==i)[0]] = count[i]/np.sum(count)\n    \n    return sample_weight\n\n\ndef one_hot_tensor(y, num_dim):\n    y_onehot = torch.zeros(y.shape[0], num_dim)\n    y_onehot.scatter_(1, y.view(-1,1), 1)\n    \n    return y_onehot\n\n\ndef cosine_distance_torch(x1, x2=None, eps=1e-8):\n    x2 = x1 if x2 is None else x2\n    w1 = x1.norm(p=2, dim=1, keepdim=True)\n    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n    return 1 - torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)\n\n\ndef to_sparse(x):\n    x_typename = torch.typename(x).split('.')[-1]\n    sparse_tensortype = getattr(torch.sparse, x_typename)\n    indices = torch.nonzero(x)\n    if len(indices.shape) == 0:  # if all elements are zeros\n        return sparse_tensortype(*x.shape)\n    indices = indices.t()\n    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n    return sparse_tensortype(indices, values, x.size())\n\n\ndef cal_adj_mat_parameter(edge_per_node, data, metric=\"cosine\"):\n    assert metric == \"cosine\", \"Only cosine distance implemented\"\n    dist = cosine_distance_torch(data, data)\n    parameter = torch.sort(dist.reshape(-1,)).values[edge_per_node*data.shape[0]]\n    return np.ndarray.item(parameter.data.cpu().numpy())\n\n\ndef graph_from_dist_tensor(dist, parameter, self_dist=True):\n    if self_dist:\n        assert dist.shape[0]==dist.shape[1], \"Input is not pairwise dist matrix\"\n    g = (dist <= parameter).float()\n    if self_dist:\n        diag_idx = np.diag_indices(g.shape[0])\n        g[diag_idx[0], diag_idx[1]] = 0\n        \n    return g\n\n\ndef gen_adj_mat_tensor(data, parameter, metric=\"cosine\"):\n    assert metric == \"cosine\", \"Only cosine distance implemented\"\n    dist = cosine_distance_torch(data, data)\n    g = graph_from_dist_tensor(dist, parameter, self_dist=True)\n    if metric == \"cosine\":\n        adj = 1-dist\n    else:\n        raise NotImplementedError\n    adj = adj*g \n    adj_T = adj.transpose(0,1)\n    I = torch.eye(adj.shape[0])\n    if cuda:\n        I = I.cuda()\n    adj = adj + adj_T*(adj_T > adj).float() - adj*(adj_T > adj).float()\n    adj = F.normalize(adj + I, p=1)\n    adj = to_sparse(adj)\n    \n    return adj\n\n\ndef gen_test_adj_mat_tensor(data, trte_idx, parameter, metric=\"cosine\"):\n    assert metric == \"cosine\", \"Only cosine distance implemented\"\n    adj = torch.zeros((data.shape[0], data.shape[0]))\n    if cuda:\n        adj = adj.cuda()\n    num_tr = len(trte_idx[\"tr\"])\n    \n    dist_tr2te = cosine_distance_torch(data[trte_idx[\"tr\"]], data[trte_idx[\"te\"]])\n    g_tr2te = graph_from_dist_tensor(dist_tr2te, parameter, self_dist=False)\n    if metric == \"cosine\":\n        adj[:num_tr,num_tr:] = 1-dist_tr2te\n    else:\n        raise NotImplementedError\n    adj[:num_tr,num_tr:] = adj[:num_tr,num_tr:]*g_tr2te\n    \n    dist_te2tr = cosine_distance_torch(data[trte_idx[\"te\"]], data[trte_idx[\"tr\"]])\n    g_te2tr = graph_from_dist_tensor(dist_te2tr, parameter, self_dist=False)\n    if metric == \"cosine\":\n        adj[num_tr:,:num_tr] = 1-dist_te2tr\n    else:\n        raise NotImplementedError\n    adj[num_tr:,:num_tr] = adj[num_tr:,:num_tr]*g_te2tr # retain selected edges\n    \n    adj_T = adj.transpose(0,1)\n    I = torch.eye(adj.shape[0])\n    if cuda:\n        I = I.cuda()\n    adj = adj + adj_T*(adj_T > adj).float() - adj*(adj_T > adj).float()\n    adj = F.normalize(adj + I, p=1)\n    adj = to_sparse(adj)\n    \n    return adj\n\n\ndef save_model_dict(folder, model_dict):\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    for module in model_dict:\n        torch.save(model_dict[module].state_dict(), os.path.join(folder, module+\".pth\"))\n            \n    \ndef load_model_dict(folder, model_dict):\n    for module in model_dict:\n        if os.path.exists(os.path.join(folder, module+\".pth\")):\n#            print(\"Module {:} loaded!\".format(module))\n            model_dict[module].load_state_dict(torch.load(os.path.join(folder, module+\".pth\"), map_location=\"cuda:{:}\".format(torch.cuda.current_device())))\n        else:\n            print(\"WARNING: Module {:} from model_dict is not loaded!\".format(module))\n        if cuda:\n            model_dict[module].cuda()    \n    return model_dict\n\n\n\ndef gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter):\n    adj_metric = \"cosine\" # cosine distance\n    adj_train_list = []\n    adj_test_list = []\n    for i in range(len(data_tr_list)):\n        adj_parameter_adaptive = cal_adj_mat_parameter(adj_parameter, data_tr_list[i], adj_metric)\n        adj_train_list.append(gen_adj_mat_tensor(data_tr_list[i], adj_parameter_adaptive, adj_metric))\n        adj_test_list.append(gen_test_adj_mat_tensor(data_trte_list[i], trte_idx, adj_parameter_adaptive, adj_metric))\n    \n    return adj_train_list, adj_test_list","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:32:58.080948Z","iopub.execute_input":"2022-11-20T13:32:58.081386Z","iopub.status.idle":"2022-11-20T13:32:58.120988Z","shell.execute_reply.started":"2022-11-20T13:32:58.081272Z","shell.execute_reply":"2022-11-20T13:32:58.116791Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"adj_parameter = 2\nadj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:42:54.148290Z","iopub.execute_input":"2022-11-20T13:42:54.148636Z","iopub.status.idle":"2022-11-20T13:42:54.685829Z","shell.execute_reply.started":"2022-11-20T13:42:54.148610Z","shell.execute_reply":"2022-11-20T13:42:54.685040Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"data_tr_list","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:42:54.687181Z","iopub.execute_input":"2022-11-20T13:42:54.687795Z","iopub.status.idle":"2022-11-20T13:42:54.700919Z","shell.execute_reply.started":"2022-11-20T13:42:54.687757Z","shell.execute_reply":"2022-11-20T13:42:54.698547Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[tensor([[0.2823, 0.1148, 0.5080,  ..., 0.3504, 0.4459, 0.2945],\n         [0.2963, 0.0542, 0.5594,  ..., 0.2358, 0.6009, 0.3951],\n         [0.2352, 0.3170, 0.5059,  ..., 0.1575, 0.4421, 0.4063],\n         ...,\n         [0.1065, 0.0217, 0.3142,  ..., 0.3878, 0.4653, 0.3595],\n         [0.2444, 0.1371, 0.5480,  ..., 0.1654, 0.6213, 0.3647],\n         [0.4102, 0.1096, 0.5818,  ..., 0.2716, 0.5736, 0.3803]]),\n tensor([[0.7615, 0.8605, 0.6642,  ..., 0.5645, 0.5360, 0.5845],\n         [0.8281, 0.8584, 0.6465,  ..., 0.6303, 0.5936, 0.2596],\n         [0.8959, 0.8155, 0.5906,  ..., 0.8908, 0.6695, 0.1482],\n         ...,\n         [0.6335, 0.5735, 0.3636,  ..., 0.4891, 0.2417, 0.1640],\n         [0.8527, 0.8258, 0.5873,  ..., 0.7349, 0.4855, 0.4430],\n         [0.8407, 0.8062, 0.6367,  ..., 0.5428, 0.3078, 0.2428]]),\n tensor([[0.7256, 0.7770, 0.7261,  ..., 0.3342, 0.4480, 0.8130],\n         [0.7236, 0.7750, 0.7238,  ..., 0.3470, 0.4269, 0.7430],\n         [0.7073, 0.7592, 0.7078,  ..., 0.2833, 0.5421, 0.7530],\n         ...,\n         [0.6208, 0.6736, 0.6228,  ..., 0.3444, 0.5028, 0.7707],\n         [0.6982, 0.7501, 0.6984,  ..., 0.2825, 0.5187, 0.7176],\n         [0.6572, 0.7086, 0.6575,  ..., 0.2891, 0.4822, 0.7292]])]"},"metadata":{}}]},{"cell_type":"code","source":"adj_tr_list[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:43:07.356181Z","iopub.execute_input":"2022-11-20T13:43:07.356541Z","iopub.status.idle":"2022-11-20T13:43:07.366447Z","shell.execute_reply.started":"2022-11-20T13:43:07.356510Z","shell.execute_reply":"2022-11-20T13:43:07.365351Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor(indices=tensor([[  0,   1,   2,  ..., 872, 873, 874],\n                       [  0,   1,   2,  ..., 872, 873, 874]]),\n       values=tensor([1.0000, 1.0000, 0.2505,  ..., 1.0000, 1.0000, 1.0000]),\n       size=(875, 875), nnz=1755, layout=torch.sparse_coo)"},"metadata":{}}]},{"cell_type":"code","source":"train_data_list = []\ntest_data_list = []","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:42:56.627190Z","iopub.execute_input":"2022-11-20T13:42:56.627564Z","iopub.status.idle":"2022-11-20T13:42:56.635152Z","shell.execute_reply.started":"2022-11-20T13:42:56.627533Z","shell.execute_reply":"2022-11-20T13:42:56.634072Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"for i in range(len(adj_tr_list)):\n    y = torch.tensor(labels_trte)\n    cur_data = Data(x=data_trte_list[i],\n                    edge_index=adj_tr_list[i].coalesce().indices(),\n                    edge_weight=adj_tr_list[i].coalesce().values(),\n                    y = y)\n    train_data_list.append(cur_data)\n    \n    '''cur_test_data = Data(x=data_trte_list[i][np.array(trte_idx['te'])],\n                    edge_index=adj_te_list[i].coalesce().indices(),\n                    edge_weight=adj_te_list[i].coalesce().values(),\n                    y = labels_trte[np.array(trte_idx['te'])])\n    test_data_list.append(cur_test_data)'''","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:30:53.411963Z","iopub.execute_input":"2022-11-08T13:30:53.412752Z","iopub.status.idle":"2022-11-08T13:30:53.426977Z","shell.execute_reply.started":"2022-11-08T13:30:53.412701Z","shell.execute_reply":"2022-11-08T13:30:53.425733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2000\nclass GConv(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, activation, num_layers):\n        super(GConv, self).__init__()\n        self.activation = activation()\n        self.layers = torch.nn.ModuleList()\n        self.layers.append(GCNConv(input_dim, hidden_dim, cached=False))\n        for _ in range(num_layers - 1):\n            self.layers.append(GCNConv(hidden_dim, hidden_dim, cached=False))\n\n    def forward(self, x, edge_index, edge_weight=None):\n        z = x\n        for i, conv in enumerate(self.layers):\n            z = conv(z, edge_index, edge_weight)\n            z = self.activation(z)\n        return z\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, encoder, augmentor, hidden_dim, proj_dim):\n        super(Encoder, self).__init__()\n        self.encoder = encoder\n        self.augmentor = augmentor\n\n        self.fc1 = torch.nn.Linear(hidden_dim, proj_dim)\n        self.fc2 = torch.nn.Linear(proj_dim, hidden_dim)\n\n    def forward(self, x, edge_index, edge_weight=None):\n        aug1, aug2 = self.augmentor\n        x1, edge_index1, edge_weight1 = aug1(x, edge_index, edge_weight)\n        x2, edge_index2, edge_weight2 = aug2(x, edge_index, edge_weight)\n        z = self.encoder(x, edge_index, edge_weight)\n        z1 = self.encoder(x1, edge_index1, edge_weight1)\n        z2 = self.encoder(x2, edge_index2, edge_weight2)\n        return z, z1, z2\n\n    def project(self, z: torch.Tensor) -> torch.Tensor:\n        z = F.elu(self.fc1(z))\n        return self.fc2(z)\n\n\ndef train(encoder_model, contrast_model, data, optimizer):\n    encoder_model.train()\n    optimizer.zero_grad()\n    z, z1, z2 = encoder_model(data.x, data.edge_index, data.edge_weight)\n    h1, h2 = [encoder_model.project(x) for x in [z1, z2]]\n    loss = contrast_model(h1, h2)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n\ndef test(encoder_model, data):\n    encoder_model.eval()\n    z, _, _ = encoder_model(data.x, data.edge_index, data.edge_weight)\n    split = get_split(num_samples=z.size()[0], train_ratio=0.6, test_ratio=0.01)\n    result = LREvaluator()(z, data.y, split)\n    return result\n\n\ndef main():\n    device = torch.device('cpu')\n    path = osp.join(osp.expanduser('~'), 'datasets')\n    #dataset = Planetoid(path, name='Cora', transform=T.NormalizeFeatures())\n    data = train_data_list[1].to(device)\n\n    aug1 = A.Compose([A.EdgeRemoving(pe=0.4), A.FeatureMasking(pf=0.4)])\n    aug2 = A.Compose([A.EdgeRemoving(pe=0.4), A.FeatureMasking(pf=0.4)])\n\n    gconv = GConv(input_dim=200, hidden_dim=100, activation=torch.nn.ReLU, num_layers=3).to(device)\n    encoder_model = Encoder(encoder=gconv, augmentor=(aug1, aug2), hidden_dim=200, proj_dim=100).to(device)\n    contrast_model = DualBranchContrast(loss=L.InfoNCE(tau=0.2), mode='L2L', intraview_negs=True).to(device)\n\n    optimizer = Adam(encoder_model.parameters(), lr=0.001)\n\n    with tqdm(total=num_epochs, desc='(T)') as pbar:\n        for epoch in range(1, num_epochs+1):\n            loss = train(encoder_model, contrast_model, data, optimizer)\n            pbar.set_postfix({'loss': loss})\n            pbar.update()\n\n    test_result = test(encoder_model, data)\n    print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n    tr = test_result\n    print(test_result)\n    \n    return encoder_model\n\nif __name__ == '__main__':\n    #e_m=main() \n    '''device = torch.device('cpu')\n    aug1 = A.Compose([A.EdgeRemoving(pe=0.3), A.FeatureMasking(pf=0.3)])\n    aug2 = A.Compose([A.EdgeRemoving(pe=0.3), A.FeatureMasking(pf=0.3)])\n\n    gconv = GConv(input_dim=200, hidden_dim=200, activation=torch.nn.ReLU, num_layers=3).to(device)\n    e_m = Encoder(encoder=gconv, augmentor=(aug1, aug2), hidden_dim=200, proj_dim=100).to(device)'''","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:30:53.428718Z","iopub.execute_input":"2022-11-08T13:30:53.429187Z","iopub.status.idle":"2022-11-08T13:30:53.467538Z","shell.execute_reply.started":"2022-11-08T13:30:53.429137Z","shell.execute_reply":"2022-11-08T13:30:53.465987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder, view_list)\nadj_parameter = 2\nadj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:07.357513Z","iopub.execute_input":"2022-11-08T13:35:07.357932Z","iopub.status.idle":"2022-11-08T13:35:10.007529Z","shell.execute_reply.started":"2022-11-08T13:35:07.357900Z","shell.execute_reply":"2022-11-08T13:35:10.005793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_te_list = []\nfor i in range(len(data_trte_list)):\n    cur_d = data_trte_list[i]\n    cur_x = cur_d[trte_idx['te']]\n    data_te_list.append(cur_x)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:10.012676Z","iopub.execute_input":"2022-11-08T13:35:10.013118Z","iopub.status.idle":"2022-11-08T13:35:10.025792Z","shell.execute_reply.started":"2022-11-08T13:35:10.013078Z","shell.execute_reply":"2022-11-08T13:35:10.023880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Graph_Data(Dataset):\n    def __init__(self,D,adj_o):\n        super().__init__()\n        self.D = D\n        self.adj_o = adj_o\n        \n    def __len__(self):\n        return 1\n    \n    def __getitem__(self,idx):\n        return self.D,self.adj_o\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:10.028268Z","iopub.execute_input":"2022-11-08T13:35:10.028960Z","iopub.status.idle":"2022-11-08T13:35:10.046533Z","shell.execute_reply.started":"2022-11-08T13:35:10.028907Z","shell.execute_reply":"2022-11-08T13:35:10.044685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"td = Graph_Data(data_tr_list[1],adj_tr_list[1])\nvd = Graph_Data(data_trte_list[1],adj_te_list[1])","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:10.049264Z","iopub.execute_input":"2022-11-08T13:35:10.050471Z","iopub.status.idle":"2022-11-08T13:35:10.059845Z","shell.execute_reply.started":"2022-11-08T13:35:10.050426Z","shell.execute_reply":"2022-11-08T13:35:10.058351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_batch(batch):\n    return batch","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:10.454221Z","iopub.execute_input":"2022-11-08T13:35:10.454717Z","iopub.status.idle":"2022-11-08T13:35:10.464160Z","shell.execute_reply.started":"2022-11-08T13:35:10.454664Z","shell.execute_reply":"2022-11-08T13:35:10.462812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tl = DataLoader(td,batch_size=1,collate_fn = collate_batch)\nvl = DataLoader(vd,batch_size=1,collate_fn = collate_batch)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:11.191225Z","iopub.execute_input":"2022-11-08T13:35:11.191696Z","iopub.status.idle":"2022-11-08T13:35:11.200916Z","shell.execute_reply.started":"2022-11-08T13:35:11.191657Z","shell.execute_reply":"2022-11-08T13:35:11.199122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = Accuracy()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:11.503752Z","iopub.execute_input":"2022-11-08T13:35:11.504418Z","iopub.status.idle":"2022-11-08T13:35:11.514059Z","shell.execute_reply.started":"2022-11-08T13:35:11.504377Z","shell.execute_reply":"2022-11-08T13:35:11.512784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitModel(pl.LightningModule):\n    def __init__(self,lr,e_m):\n        super().__init__()\n        self.layers = [torch.nn.Linear(200,100),\n                       torch.nn.LeakyReLU(),\n                       torch.nn.Dropout(p=0.3),\n                       torch.nn.Linear(100,50),\n                       torch.nn.LeakyReLU(),\n                       torch.nn.Dropout(p=0.3),\n                       torch.nn.Linear(50,2)]\n        self.model = torch.nn.Sequential(*self.layers)\n        self.lr = lr\n        self.e_m = e_m\n\n    def forward(self, x,edge_index,edge_weight):\n        return self.e_m(x,edge_index,edge_weight)\n\n    def training_step(self, batch, batch_idx):\n        d_obj, adj_obj = batch[0]\n        z,_,_ = self(d_obj,adj_obj.coalesce().indices(),adj_obj.coalesce().values())\n        nn_logits = self.model(z)\n        y = torch.tensor(labels_trte[trte_idx['tr']]).long()\n        loss = F.cross_entropy(nn_logits, y)\n        acc = accuracy(nn_logits,y)\n        self.log(\"Training_Loss\",loss)\n        self.log(\"Training_Accuracy\",acc)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        d_obj, adj_obj = batch[0]\n        z,_,_ = self(d_obj,adj_obj.coalesce().indices(),adj_obj.coalesce().values())\n        nn_logits = self.model(z)[trte_idx['te']]\n        y = torch.tensor(labels_trte[trte_idx['te']]).long()\n        loss = F.cross_entropy(nn_logits, y)\n        acc = accuracy(nn_logits,y)\n        self.log(\"Validation_Loss\",loss)\n        self.log(\"Validation_Accuracy\",acc)\n        \n        \n        return loss\n    \n    \n    def test_step(self, batch, batch_idx):\n        d_obj, adj_obj = batch[0]\n        z,_,_ = self(d_obj,adj_obj.coalesce().indices(),adj_obj.coalesce().values())\n        nn_logits = self.model(z)[trte_idx['te']]\n        y = torch.tensor(labels_trte[trte_idx['te']]).long()\n        loss = F.cross_entropy(nn_logits, y)\n        acc = accuracy(nn_logits,y)\n        test_acc.append(acc)\n        self.log(\"Test_Loss\",loss)\n        self.log(\"Test_Accuracy\",acc)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:11.917250Z","iopub.execute_input":"2022-11-08T13:35:11.918555Z","iopub.status.idle":"2022-11-08T13:35:11.938913Z","shell.execute_reply.started":"2022-11-08T13:35:11.918495Z","shell.execute_reply":"2022-11-08T13:35:11.937886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sup_model = LitModel(0.001,e_m)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:12.206422Z","iopub.execute_input":"2022-11-08T13:35:12.207388Z","iopub.status.idle":"2022-11-08T13:35:12.215202Z","shell.execute_reply.started":"2022-11-08T13:35:12.207336Z","shell.execute_reply":"2022-11-08T13:35:12.213793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''trainer = Trainer(accelerator = 'cpu',\n                  logger=wandb_logger,\n                  max_epochs=100,\n                  log_every_n_steps=1)'''","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:12.481859Z","iopub.execute_input":"2022-11-08T13:35:12.482259Z","iopub.status.idle":"2022-11-08T13:35:12.491860Z","shell.execute_reply.started":"2022-11-08T13:35:12.482227Z","shell.execute_reply":"2022-11-08T13:35:12.490602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''trainer.fit(sup_model,tl,vl)\ntrainer.test(sup_model,vl)\nwandb.finish()'''","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:35:12.724298Z","iopub.execute_input":"2022-11-08T13:35:12.725269Z","iopub.status.idle":"2022-11-08T13:35:12.735757Z","shell.execute_reply.started":"2022-11-08T13:35:12.725219Z","shell.execute_reply":"2022-11-08T13:35:12.734475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____________________________________________________________","metadata":{}},{"cell_type":"code","source":"num_epochs = 2000\nclass GConv(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, activation, num_layers):\n        super(GConv, self).__init__()\n        self.activation = activation()\n        self.layers = torch.nn.ModuleList()\n        self.layers.append(GCNConv(input_dim, hidden_dim, cached=False))\n        for _ in range(num_layers - 1):\n            self.layers.append(GCNConv(hidden_dim, hidden_dim, cached=False))\n\n    def forward(self, x, edge_index, edge_weight=None):\n        z = x\n        for i, conv in enumerate(self.layers):\n            z = conv(z, edge_index, edge_weight)\n            z = self.activation(z)\n            if i != len(self.layers)-1:\n                z = F.dropout(z,p=0.1)\n        return z\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, encoder, augmentor, hidden_dim, proj_dim):\n        super(Encoder, self).__init__()\n        self.encoder = encoder\n        self.augmentor = augmentor\n\n        self.fc1 = torch.nn.Linear(hidden_dim, proj_dim)\n        self.fc2 = torch.nn.Linear(proj_dim, hidden_dim)\n\n    def forward(self, x, edge_index, edge_weight=None):\n        aug1, aug2 = self.augmentor\n        x1, edge_index1, edge_weight1 = aug1(x, edge_index, edge_weight)\n        x2, edge_index2, edge_weight2 = aug2(x, edge_index, edge_weight)\n        z = self.encoder(x, edge_index, edge_weight)\n        z1 = self.encoder(x1, edge_index1, edge_weight1)\n        z2 = self.encoder(x2, edge_index2, edge_weight2)\n        return z, z1, z2\n\n    def project(self, z: torch.Tensor) -> torch.Tensor:\n        z = F.elu(self.fc1(z))\n        return self.fc2(z)\n\n\ndef train(encoder_model, contrast_model, data, optimizer):\n    encoder_model.train()\n    optimizer.zero_grad()\n    z, z1, z2 = encoder_model(data.x, data.edge_index, data.edge_weight)\n    h1, h2 = [encoder_model.project(x) for x in [z1, z2]]\n    loss = contrast_model(h1, h2)\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n\n\ndef test(encoder_model, data):\n    encoder_model.eval()\n    z, _, _ = encoder_model(data.x, data.edge_index, data.edge_weight)\n    split = get_split(num_samples=z.size()[0], train_ratio=0.6, test_ratio=0.01)\n    result = LREvaluator()(z, data.y, split)\n    return result\n\n\ndef main(model_no):\n    \n    device = torch.device('cpu')\n    path = osp.join(osp.expanduser('~'), 'datasets')\n    #dataset = Planetoid(path, name='Cora', transform=T.NormalizeFeatures())\n    data = train_data_list[model_no].to(device)\n\n    aug1 = A.Compose([A.EdgeRemoving(pe=0.4), A.FeatureMasking(pf=0.4)])\n    aug2 = A.Compose([A.EdgeRemoving(pe=0.4), A.FeatureMasking(pf=0.4)])\n    \n    if model_no == 2:\n        gconv = GConv(input_dim=503, hidden_dim=100, activation=torch.nn.ReLU, num_layers=1).to(device)\n        encoder_model = Encoder(encoder=gconv, augmentor=(aug1, aug2), hidden_dim=100, proj_dim=100).to(device)\n        contrast_model = DualBranchContrast(loss=L.InfoNCE(tau=0.2), mode='L2L', intraview_negs=True).to(device)\n        \n    else:\n        gconv = GConv(input_dim=1000, hidden_dim=100, activation=torch.nn.ReLU, num_layers=1).to(device)\n        encoder_model = Encoder(encoder=gconv, augmentor=(aug1, aug2), hidden_dim=100, proj_dim=100).to(device)\n        contrast_model = DualBranchContrast(loss=L.InfoNCE(tau=0.2), mode='L2L', intraview_negs=True).to(device)\n\n    optimizer = Adam(encoder_model.parameters(), lr=0.0001)\n\n    with tqdm(total=num_epochs, desc='(T)') as pbar:\n        for epoch in range(1, num_epochs+1):\n            loss = train(encoder_model, contrast_model, data, optimizer)\n            pbar.set_postfix({'loss': loss})\n            pbar.update()\n\n    '''test_result = test(encoder_model, data)\n    print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n    tr = test_result\n    print(test_result)'''\n    \n    return encoder_model\n\nif __name__ == '__main__':\n    e_m_MRNA=main(0) \n    e_m_meth=main(1)\n    e_m_miRNA=main(2)\n    '''device = torch.device('cpu')\n    aug1 = A.Compose([A.EdgeRemoving(pe=0.3), A.FeatureMasking(pf=0.3)])\n    aug2 = A.Compose([A.EdgeRemoving(pe=0.3), A.FeatureMasking(pf=0.3)])\n\n    gconv = GConv(input_dim=200, hidden_dim=200, activation=torch.nn.ReLU, num_layers=3).to(device)\n    e_m = Encoder(encoder=gconv, augmentor=(aug1, aug2), hidden_dim=200, proj_dim=100).to(device)'''","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:44:02.531128Z","iopub.execute_input":"2022-11-08T13:44:02.531545Z","iopub.status.idle":"2022-11-08T13:52:13.131765Z","shell.execute_reply.started":"2022-11-08T13:44:02.531513Z","shell.execute_reply":"2022-11-08T13:52:13.130278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = Accuracy()\nf1 = F1Score(num_classes = 5,average = 'weighted')\nauroc = F1Score(num_classes=5,average = 'macro')","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:52:15.373835Z","iopub.execute_input":"2022-11-08T13:52:15.374235Z","iopub.status.idle":"2022-11-08T13:52:15.387652Z","shell.execute_reply.started":"2022-11-08T13:52:15.374201Z","shell.execute_reply":"2022-11-08T13:52:15.386167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitModel(pl.LightningModule):\n    ''' torch.nn.Linear(100,50),\n        torch.nn.LeakyReLU(),\n        torch.nn.Dropout(p=0.3),'''\n    def __init__(self,lr,e_m_0,e_m_1,e_m_2):\n        super().__init__()\n        self.layers = [torch.nn.Linear(300,100),\n                       torch.nn.LeakyReLU(),\n                       torch.nn.Dropout(p=0.1),\n                       torch.nn.Linear(100,5)]\n        self.model = torch.nn.Sequential(*self.layers)\n        self.lr = lr\n        self.e_m_mRNA = e_m_0\n        self.e_m_meth = e_m_1\n        self.e_m_miRNA = e_m_2\n\n    def forward(self, x,model_no,edge_index,edge_weight):\n        if model_no == 0:\n            return self.e_m_mRNA(x,edge_index,edge_weight)\n        elif model_no == 1:\n            return self.e_m_meth(x,edge_index,edge_weight)\n        elif model_no == 2:\n            return self.e_m_miRNA(x,edge_index,edge_weight)\n        else:\n            return \"Wrong Model\"\n\n    def training_step(self, batch, batch_idx):\n        d_obj_0, adj_obj_0,d_obj_1, adj_obj_1,d_obj_2, adj_obj_2 = batch[0]\n        z_0,_,_ = self(d_obj_0,0,adj_obj_0.coalesce().indices(),adj_obj_0.coalesce().values())\n        z_1,_,_ = self(d_obj_1,1,adj_obj_1.coalesce().indices(),adj_obj_1.coalesce().values())\n        z_2,_,_ = self(d_obj_2,2,adj_obj_2.coalesce().indices(),adj_obj_2.coalesce().values())\n        z = torch.cat([z_0,z_1,z_2],axis=1)\n        nn_logits = self.model(z)\n        y = torch.tensor(labels_trte[trte_idx['tr']]).long()\n        loss = F.cross_entropy(nn_logits, y)\n        acc = accuracy(nn_logits,y)\n        f1_s = f1(nn_logits,y)\n        auc_s = auroc(nn_logits,y)\n        self.log(\"Training_Loss\",loss)\n        self.log(\"Training_Accuracy\",acc)\n        self.log(\"Training_F1_weighted\",f1_s)\n        self.log(\"Training_F1_macro\",auc_s)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        d_obj_0, adj_obj_0,d_obj_1, adj_obj_1,d_obj_2, adj_obj_2 = batch[0]\n        z_0,_,_ = self(d_obj_0,0,adj_obj_0.coalesce().indices(),adj_obj_0.coalesce().values())\n        z_1,_,_ = self(d_obj_1,1,adj_obj_1.coalesce().indices(),adj_obj_1.coalesce().values())\n        z_2,_,_ = self(d_obj_2,2,adj_obj_2.coalesce().indices(),adj_obj_2.coalesce().values())\n        z = torch.cat([z_0,z_1,z_2],axis=1)\n        nn_logits = self.model(z)[trte_idx['te']]\n        y = torch.tensor(labels_trte[trte_idx['te']]).long()\n        loss = F.cross_entropy(nn_logits, y)\n        acc = accuracy(nn_logits,y)\n        f1_s = f1(nn_logits,y)\n        auc_s = auroc(nn_logits,y)\n        self.log(\"Validation_Loss\",loss)\n        self.log(\"Validation_Accuracy\",acc)\n        self.log(\"Validation_F1_weighted\",f1_s)\n        self.log(\"Validation_F1_macro\",auc_s)\n        \n        \n        return loss\n    \n    \n    def test_step(self, batch, batch_idx):\n        d_obj_0, adj_obj_0,d_obj_1, adj_obj_1,d_obj_2, adj_obj_2 = batch[0]\n        z_0,_,_ = self(d_obj_0,0,adj_obj_0.coalesce().indices(),adj_obj_0.coalesce().values())\n        z_1,_,_ = self(d_obj_1,1,adj_obj_1.coalesce().indices(),adj_obj_1.coalesce().values())\n        z_2,_,_ = self(d_obj_2,2,adj_obj_2.coalesce().indices(),adj_obj_2.coalesce().values())\n        z = torch.cat([z_0,z_1,z_2],axis=1)\n        nn_logits = self.model(z)[trte_idx['te']]\n        y = torch.tensor(labels_trte[trte_idx['te']]).long()\n        loss = F.cross_entropy(nn_logits, y)\n        acc = accuracy(nn_logits,y)\n        f1_s = f1(nn_logits,y)\n        auc_s = auroc(nn_logits,y)\n        self.log(\"Test_Loss\",loss)\n        self.log(\"Test_Accuracy\",acc)\n        self.log(\"Test_F1_weighted\",f1_s)\n        self.log(\"Test_F1_macro\",auc_s)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.lr)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:55:22.731181Z","iopub.execute_input":"2022-11-08T13:55:22.731589Z","iopub.status.idle":"2022-11-08T13:55:22.760236Z","shell.execute_reply.started":"2022-11-08T13:55:22.731556Z","shell.execute_reply":"2022-11-08T13:55:22.759162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Graph_Data(Dataset):\n    def __init__(self,D,adj_o):\n        super().__init__()\n        self.D = D\n        self.adj_o = adj_o\n        \n    def __len__(self):\n        return 1\n    \n    def __getitem__(self,idx):\n        return self.D[0],self.adj_o[0],self.D[1],self.adj_o[1],self.D[2],self.adj_o[2]\n        \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:55:23.374016Z","iopub.execute_input":"2022-11-08T13:55:23.374982Z","iopub.status.idle":"2022-11-08T13:55:23.385158Z","shell.execute_reply.started":"2022-11-08T13:55:23.374942Z","shell.execute_reply":"2022-11-08T13:55:23.383653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_tr_list, data_trte_list, trte_idx, labels_trte = prepare_trte_data(data_folder, view_list)\nadj_parameter = 2\nadj_tr_list, adj_te_list = gen_trte_adj_mat(data_tr_list, data_trte_list, trte_idx, adj_parameter)","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:40:37.505923Z","iopub.execute_input":"2022-11-20T13:40:37.506290Z","iopub.status.idle":"2022-11-20T13:40:39.711147Z","shell.execute_reply.started":"2022-11-20T13:40:37.506263Z","shell.execute_reply":"2022-11-20T13:40:39.709520Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"adj_tr_list[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-20T13:37:35.159621Z","iopub.execute_input":"2022-11-20T13:37:35.159939Z","iopub.status.idle":"2022-11-20T13:37:35.171502Z","shell.execute_reply.started":"2022-11-20T13:37:35.159908Z","shell.execute_reply":"2022-11-20T13:37:35.169949Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"tensor(indices=tensor([[  0,   1,   2,  ..., 609, 610, 611],\n                       [  0,   1,   2,  ..., 609, 610, 611]]),\n       values=tensor([1.0000, 1.0000, 0.2505,  ..., 1.0000, 1.0000, 1.0000]),\n       size=(612, 612), nnz=1226, layout=torch.sparse_coo)"},"metadata":{}}]},{"cell_type":"code","source":"td = Graph_Data(data_tr_list,adj_tr_list)\nvd = Graph_Data(data_trte_list,adj_te_list)\ndef collate_batch(batch):\n    return batch\n\ntl = DataLoader(td,batch_size=1,collate_fn = collate_batch)\nvl = DataLoader(vd,batch_size=1,collate_fn = collate_batch)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:55:26.179746Z","iopub.execute_input":"2022-11-08T13:55:26.180235Z","iopub.status.idle":"2022-11-08T13:55:26.188014Z","shell.execute_reply.started":"2022-11-08T13:55:26.180199Z","shell.execute_reply":"2022-11-08T13:55:26.186763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp_model = LitModel(0.00001,e_m_MRNA,e_m_meth,e_m_miRNA)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:55:26.189688Z","iopub.execute_input":"2022-11-08T13:55:26.190466Z","iopub.status.idle":"2022-11-08T13:55:26.203663Z","shell.execute_reply.started":"2022-11-08T13:55:26.190418Z","shell.execute_reply":"2022-11-08T13:55:26.202358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_1 = Trainer(accelerator = 'cpu',\n                  logger=wandb_logger,\n                  max_epochs=7111,\n                  log_every_n_steps=1)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:55:26.206430Z","iopub.execute_input":"2022-11-08T13:55:26.206791Z","iopub.status.idle":"2022-11-08T13:55:26.482497Z","shell.execute_reply.started":"2022-11-08T13:55:26.206757Z","shell.execute_reply":"2022-11-08T13:55:26.480846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_1.fit(comp_model,tl,vl)\ntrainer_1.test(comp_model,vl)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-11-08T13:55:26.485250Z","iopub.execute_input":"2022-11-08T13:55:26.485857Z","iopub.status.idle":"2022-11-08T14:18:57.976150Z","shell.execute_reply.started":"2022-11-08T13:55:26.485796Z","shell.execute_reply":"2022-11-08T14:18:57.974937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}